# -*- coding: utf-8 -*-
"""titanic model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WBJxn1e1jgLF7glJJrPE0ElSj2TmRzVK
"""

# SHIFT + ENTER executa e cria nova linha de execução
# ferramentas:
# www.kaggle.com
# https://colab.research.google.com
#  PYTHON, Pandas, Numpy
# https://www.kaggle.com/tedllh/titanic-train
import pandas as pd
import numpy as np
# algorítmo de classificação para construção do modelo
from sklearn.ensemble import RandomForestClassifier
# função auxiliar que ajuda a separar o banco de dados em treino e validação
from sklearn.model_selection import train_test_split

dados = pd.read_csv('/content/train.csv')

dados.head()

# retira informações desnecessárias para o modelo
dados = dados.drop(['Name', 'Ticket', 'Cabin', 'Embarked'], axis=1)

dados.head()

# editando chave e variável resposta
dados = dados.set_index(['PassengerId'])
dados = dados.rename(columns={'Survived': 'target'}, inplace=False)

# compare com a execução anterior deste mesmo método para entender melhor
# redefine o indice da base de dados
# renomeia o alvo / objetivo / meta da analise
dados.head()

# descrição geral da tabela
dados.describe()

# inclui variáveis não numéricas. argumento em MAIÚSCULO
dados.describe(include=['O'])

# a maioria dos algorítmos de machine learning não aceitam dados categóricos (strings).
# trocando o que é feminino por 1 e masculino por 0
# criar 3 colunas. uma para cada classe.
# quando Pclass == 1, terá 1 nesta coluna e zero nas outras.
# quando Pclass == 2, terá 1 nesta coluna e zero nas outras.
# quando Pclass == 3, terá 1 nesta coluna e zero nas outras.
dados['Sex_F'] = np.where(dados['Sex'] == 'female', 1, 0)
dados['Pclass_1'] = np.where(dados['Pclass'] == 1, 1, 0)
dados['Pclass_2'] = np.where(dados['Pclass'] == 2, 1, 0)
dados['Pclass_3'] = np.where(dados['Pclass'] == 3, 1, 0)

#  remove as variáveis que foram derivadas.
dados = dados.drop(['Pclass', 'Sex'], axis=1)

dados.head()

# informa se a tabela tem columas com valores que não foram preenchidos (missing).
# no caso age tem 177 valores não preenchidos
dados.isnull().sum()

# substituie valores
dados.fillna(0, inplace=True)

# substitue valores missing da coluna age por zero. poderia trocar por uma media de idade por exemplo.
dados.isnull().sum()

# definindo porcentagem de treino e de teste
# algorítimo de amostragem
# segmentar base de dados totais e testes e amostras
# 70% para treino do modelo e 30% para testes do modelo
# porque não treinar o modelo inteiro? RESPOSTA:
# para que o modelo não fique tão específico para a base como um todo, mas que funcione também para novas informações
# retira-se uma uma parte do banco de dados para que o modelo não veja estas informações.
# essa parte retirada será avaliada depois como se estivesse simulando um dado produtivo
# separa a base e como resposta tem uma tabela de treino e outra para teste.
x_train, x_test, y_train, y_test = train_test_split(dados.drop(['target'], axis=1),
                                                    dados["target"],
                                                    test_size = 0.3,
                                                    random_state = 1234)
[{'treino': x_train.shape}, {'teste': x_test.shape}]

# modelo escolhido foi o Random Forest

# linha abaixo faz o mapeamento do modelo
rndforest = RandomForestClassifier(n_estimators=1000,
                                   criterion='gini',
                                   max_depth=5)

# linha abaixo faz o cálculo do modelo
rndforest.fit(x_train, y_train)

# calcula as chances de sobrevivencia e depois classifica.
probabilidade = rndforest.predict_proba(dados.drop('target', axis=1))[:,1]
classificacao = rndforest.predict(dados.drop('target', axis=1))

# cria coluna de probabilidade e de classificação
# 
dados['probabilidade'] = probabilidade
dados['classificacao'] = classificacao

# não existe modelo ruim. existe modelo com pouca informação.
# para melhorar este modelo basta adicionar mais dados/atributos/informações.
dados

#